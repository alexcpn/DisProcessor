{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "mount_file_id": "1J_HnTc8qzCRW-oCJwCEwvuQ9mnSV3lxq",
      "authorship_tag": "ABX9TyNZtr5e8lbaZQSSmv+kvBpQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexcpn/DisProcessor/blob/master/MultiHeadAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"LearnTransformer\n",
        "## Learning Transformers by Doing\n",
        "\n",
        "Based on my Colab file at\n",
        "    https://colab.research.google.com/drive/1qvaWLJCenxxTcKjHksHGicxdbZDdsm7i\n",
        "\n",
        "Author: Alex Punnen and ChatGPT,CoPilot\n",
        "\n",
        "Lets see how simple self attention works by writing a single headed attention and then training them on our small dataset.\n",
        "\"\"\"\n",
        "\n",
        "# Use bpe to tokenise the sence\n",
        "\n",
        "\"\"\"It all starts with a Tokenizer that breaks words to a smaller set and creates a fixed set of vocabulary. Why fixed set vocabulary, because that is finally what is used for prediction. The model is trained to output the probability of the occurance of just the next token in say a 2000 set vocabulary. The highest probability item in that set gets selected as the next. Hence the need for a constant and fixes set vocabulary\n",
        "\n",
        "In the LLAMA Paper they are using SentencePeiece tokenize\n",
        "\n",
        "*We tokenize the data with the bytepair encoding (BPE) algorithm (Sennrich et al.,2015), using the implementation from SentencePiece\n",
        "Notably, we split all numbers into individual digits, and fallback to bytes to decompose unknown UTF-8 characters.*\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "l1JRp3qMyLmu",
        "outputId": "3ba6f42d-215d-4cfc-a46f-8afb586e7724"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'It all starts with a Tokenizer that breaks words to a smaller set and creates a fixed set of vocabulary. Why fixed set vocabulary, because that is finally what is used for prediction. The model is trained to output the probability of the occurance of just the next token in say a 2000 set vocabulary. The highest probability item in that set gets selected as the next. Hence the need for a constant and fixes set vocabulary\\n\\nIn the LLAMA Paper they are using SentencePeiece tokenize\\n\\n*We tokenize the data with the bytepair encoding (BPE) algorithm (Sennrich et al.,2015), using the implementation from SentencePiece\\nNotably, we split all numbers into individual digits, and fallback to bytes to decompose unknown UTF-8 characters.*\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install --upgrade sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG3o0V7xyNfG",
        "outputId": "2083c24b-8d3f-4ac4-f11a-af02fa552b30"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# configure logging\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import sentencepiece as spm\n",
        "from datasets import load_dataset\n",
        "import math\n",
        "import logging as log\n",
        "import os\n",
        "import gc\n",
        "\n",
        "outfile='multihead_transformer.log'\n",
        "log.basicConfig(level=log.INFO,\n",
        "                format='%(asctime)s - %(message)s',\n",
        "                datefmt='%d-%b-%y %H:%M:%S',\n",
        "                handlers=[\n",
        "                    log.FileHandler(outfile),\n",
        "                    log.StreamHandler()\n",
        "                ],\n",
        "                force=True,\n",
        "                )"
      ],
      "metadata": {
        "id": "cqzlyVtIyYpv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test log\n",
        "log.info(\"test log\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMWjPMd0yaxO",
        "outputId": "52fecc7f-b686-4b55-d733-0414a548e9d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "07-Feb-25 10:55:10 - test log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkSYtkJ7xls3",
        "outputId": "db2bdda4-7de7-42bd-e460-112dea982877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "07-Feb-25 12:22:01 - Length of trainig data is  2119719\n",
            "07-Feb-25 12:22:05 - Limiting training legth to 100000\n",
            "07-Feb-25 12:22:05 - Training Non contextual tokeniser\n",
            "07-Feb-25 12:22:15 - Sentence: The Cat sat on the Fence\n",
            "07-Feb-25 12:22:15 - Tokens:  ['The▁', 'C', 'at▁', 'sat▁', 'on▁', 'the▁', 'F', 'en', 'ce▁']\n",
            "07-Feb-25 12:22:15 - Token IDs: [60, 1947, 50, 1134, 56, 16, 1945, 23, 123]\n",
            "07-Feb-25 12:22:15 - Token ids already present in file\n",
            "07-Feb-25 12:22:22 - Total tokens:  24471344\n",
            "07-Feb-25 12:22:22 - Resizing input_ids...\n",
            "07-Feb-25 12:22:23 - input_ids.shape=torch.Size([1, 24471344])\n",
            "07-Feb-25 12:22:23 - New shape:= torch.Size([24471, 1000])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Load the small dataset for training our tiny language model\n",
        "ds = load_dataset(\"roneneldan/TinyStories\")\n",
        "train_size =100000\n",
        "# use the dataset as text for training\n",
        "log.info(f\"Length of trainig data is  {len(ds['train']['text'])}\")\n",
        "# use half of this training data text\n",
        "trainingdata = ds['train']['text'][:train_size]\n",
        "log.info(f\"Limiting training legth to {len(trainingdata)}\")\n",
        "\n",
        "# 1) Write the list to a file.\n",
        "with open(\"train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in trainingdata:\n",
        "        # replace newline with space to keep each original text chunk on a single line\n",
        "        #replace special characters\n",
        "        line = line.replace(\"â€\", \"\")\n",
        "        f.write(line.replace(\"\\n\", \" \") + \"\\n\")\n",
        "# for vocabulary training\n",
        "# trainingdata2 = ds['train']['text'][:10000]\n",
        "# with open(\"vocabtrain.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "#     for line in trainingdata2:\n",
        "#         # replace newline with space to keep each original text chunk on a single line\n",
        "#         f.write(line.replace(\"\\n\", \" \") + \"\\n\")\n",
        "test_sentence = \"The Cat sat on the Fence\"\n",
        "# We use a small vocab_size just for demo. LLaMA uses a much larger vocabulary (32k tokens).\n",
        "vocab_size = 2000\n",
        "\n",
        "# if file is not there\n",
        "# this creates a vocab file and a model file\n",
        "log.info(\"Training Non contextual tokeniser\")\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input=\"train.txt\",   # our training data\n",
        "    model_prefix='llama_like',\n",
        "    vocab_size=vocab_size,\n",
        "    model_type='bpe',\n",
        "    character_coverage=1.0,\n",
        "    max_sentence_length=2048,\n",
        "    treat_whitespace_as_suffix=True,\n",
        "    split_digits=True               # This forces splitting \"123\" -> \"1\", \"2\", \"3\"\n",
        ")\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"llama_like.model\")\n",
        "\n",
        "tokens = sp.encode(test_sentence, out_type=str)\n",
        "token_ids = sp.encode(test_sentence, out_type=int)\n",
        "\n",
        "log.info(f\"Sentence: {test_sentence}\")\n",
        "log.info(f\"Tokens:  {tokens}\")\n",
        "log.info(f\"Token IDs: {token_ids}\")\n",
        "\n",
        "# get the vocabulary dictionary mapping\n",
        "# print(sp.id_to_piece(60))\n",
        "\n",
        "# Part 2\n",
        "\n",
        "# Step 1: Prepare the data for training the Attention layer\n",
        "\n",
        "# Now lets tokenise the entire text and generate a map of input_ids\n",
        "all_token_ids = []\n",
        "\n",
        "if not os.path.isfile(\"token_ids.txt\"):\n",
        "    log.info(\"Tokenizing text...\")\n",
        "    with open(\"train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            # Encode each line to token IDs\n",
        "            line_ids = sp.encode(line, out_type=int)\n",
        "            # Append them, maybe add a special token like <eol> if desired\n",
        "            all_token_ids.extend(line_ids)\n",
        "            # all_token_ids.append(eol_id)  # If you have a special EOL token\n",
        "    # Write token IDs to file\n",
        "    with open(\"token_ids.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for token_id in all_token_ids:\n",
        "            f.write(f\"{token_id}\\n\")\n",
        "else:\n",
        "    log.info(\"Token ids already present in file\")\n",
        "    #read token ids from file\n",
        "    all_token_ids = []\n",
        "    with open(\"token_ids.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            all_token_ids.append(int(line))\n",
        "\n",
        "log.info(f\"Total tokens:  {len(all_token_ids)}\")\n",
        "\n",
        "# Lets resize the input ids for training\n",
        "\n",
        "log.info(\"Resizing input_ids...\")\n",
        "\n",
        "# convert these to torch tensor\n",
        "input_ids = torch.tensor(all_token_ids, dtype=torch.long).unsqueeze(0)\n",
        "log.info(f\"input_ids.shape={input_ids.shape}\")\n",
        "# shape these (torch.Size([1, 380627])) chunk to batchsize of 1 and length of 50\n",
        "seq_length = 1000\n",
        "input_ids = input_ids.squeeze(0)  # Remove batch dim, now shape = (380627,)\n",
        "# How many 50-token chunks we can make\n",
        "num_chunks = input_ids.shape[0] // seq_length\n",
        "\n",
        "# Truncate to nearest multiple of 50\n",
        "input_ids = input_ids[:num_chunks * seq_length]\n",
        "# Reshape to (num_chunks, 50), each row is a sequence of 50 tokens\n",
        "input_ids = input_ids.view(num_chunks, seq_length)\n",
        "\n",
        "log.info(f\"New shape:= {input_ids.shape}\")  # Should be (num_chunks, 50)\n",
        "\n",
        "# this will be same as labels\n",
        "labels = input_ids.clone()\n",
        "vocab_size = 2000\n",
        "d_model = 512  # embediding size\n",
        "d_k = 64  # attention size\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to add positional encoding to the input_ids\n",
        "# Positional encoding is a way to provide the model with information about the position of each token in the sequence.\n",
        "# This is important because the model has no inherent sense of order in the tokens, since it only sees them as embeddings.\n",
        "# generated by LLM\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len):\n",
        "        super().__init__()\n",
        "        # Create a long enough 'pe' matrix of shape [max_len, d_model]\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() *\n",
        "            (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        # Even indices (2i) -> sine\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Odd indices (2i+1) -> cosine\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register as a buffer so it's moved to GPU automatically if needed\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_len, d_model)\n",
        "        We add positional encoding up to seq_len from the precomputed 'pe'.\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        # pe[:seq_len] -> shape [seq_len, d_model]\n",
        "        # We unsqueeze(0) so that shape becomes [1, seq_len, d_model],\n",
        "        # allowing addition to x which is [batch_size, seq_len, d_model].\n",
        "        x = x + self.pe[:seq_len, :].unsqueeze(0)\n",
        "        return x\n",
        "\n",
        "\n",
        "\"\"\"### Step: Adding in a Simple Attention Class\"\"\"\n",
        "\n",
        "\n",
        "class SingleHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        \"\"\"\n",
        "        d_model: dimension for Q, K, V\n",
        "        use_output_proj: if True, applies a final linear W_O\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "          Forward lyer of SingleHeadedAttention\n",
        "        \"\"\"\n",
        "        B, seq_len, d_model = x.shape  # B is batch size , seq_len is the length of the sequence , and d_model is the embedding size (512) # torch.Size([1, 999, 512])\n",
        "\n",
        "        Q = self.W_Q(x)\n",
        "        K = self.W_K(x)\n",
        "        V = self.W_V(x)\n",
        "\n",
        "        attention = torch.matmul(Q, K.transpose(-2, -1)) / \\\n",
        "            torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
        "        # Apply the mask to the attention scores\n",
        "        # why is this needed; basically it allows the model from attending to only tokens in the past, that is in the left side of\n",
        "        # the current token, when you mulitply by V\n",
        "        # the left side becomes the lower triangular matrix; and right side the future tokens are  the upper triangular matrix\n",
        "        # We build an upper-triangular mask (set to -inf) that zeros out attention (the next softmmax layer will set it to zero)\n",
        "        causal_mask = torch.triu(\n",
        "            torch.ones((seq_len, seq_len), device=x.device), diagonal=1\n",
        "        ).bool()\n",
        "        attention = attention.masked_fill(causal_mask, float('-inf'))\n",
        "        attention = torch.softmax(attention, dim=-1)\n",
        "        score = torch.matmul(attention, V)\n",
        "        # ----- [1] Add residual connection ----- ttodo take this out\n",
        "        out = x + score # without this the model output is not good\n",
        "        return out, attention\n"
      ],
      "metadata": {
        "id": "ihglfaDPyGul"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log.info(f\"vocab_size={vocab_size} embedding_dim/d_model={d_model}\")\n",
        "\n",
        "# Intialise all the layers\n",
        "\n",
        "# add in the embdeiing part from previous layer\n",
        "token_embedding = nn.Embedding(\n",
        "    num_embeddings=vocab_size, embedding_dim=d_model)\n",
        "pos_encoding = PositionalEncoding(d_model, max_len=seq_length)\n",
        "# add in the attention layer\n",
        "\n",
        "# Add a linear layer for prediction\n",
        "num_heads=2 # work on T4 GPU\n",
        "num_heads=12 # work on A100 GPU\n",
        "multihead_attention = nn.ModuleList()\n",
        "for _ in range(num_heads):\n",
        "    attention_mod = SingleHeadSelfAttention(d_model)\n",
        "    multihead_attention.append(attention_mod)\n",
        "\n",
        "prediction_layer1 = nn.Linear(d_model*num_heads, vocab_size) # as we are concatenating the heads output\n",
        "layer_norm1 = nn.LayerNorm(vocab_size)\n",
        "prediction_layer2 = nn.Linear(vocab_size, vocab_size)\n",
        "layer_norm2 = nn.LayerNorm(vocab_size) # last dimension is the vocab size\n",
        "\n",
        "\n",
        "# Define the loss function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "log.info(f\"Length of input ids ={len(input_ids)}\")\n",
        "\n",
        "# We'll combine these into a simple pipeline\n",
        "model = nn.ModuleList([token_embedding, pos_encoding,\n",
        "                      multihead_attention,layer_norm1,layer_norm2,prediction_layer1,prediction_layer2])\n",
        "\n",
        "# The most important part is the Stochastic Gradient Descent part\n",
        "# Using model.parameters() in optimizer.step() ensures all layers, including token_embedding, attention_mod, and prediction_layer, are updated\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4) # SGD is unstable and hence we use this\n",
        "\n",
        "# with higher learning loss is Nan\n",
        "\n",
        "assert False == torch.isnan(input_ids).any()\n",
        "assert False == torch.isinf(input_ids).any()\n",
        "\n",
        "# Place all in GPU\n",
        "token_embedding.to('cuda')\n",
        "pos_encoding.to('cuda')\n",
        "attention_mod.to('cuda')\n",
        "prediction_layer1.to('cuda')\n",
        "prediction_layer2.to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-WAUWeNyAIN",
        "outputId": "ccf5c832-73c4-43f5-9ffa-2b60324646a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "07-Feb-25 12:22:24 - vocab_size=2000 embedding_dim/d_model=512\n",
            "07-Feb-25 12:22:24 - Length of input ids =24471\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModuleList(\n",
              "  (0): Embedding(2000, 512)\n",
              "  (1): PositionalEncoding()\n",
              "  (2): ModuleList(\n",
              "    (0-11): 12 x SingleHeadSelfAttention(\n",
              "      (W_Q): Linear(in_features=512, out_features=512, bias=False)\n",
              "      (W_K): Linear(in_features=512, out_features=512, bias=False)\n",
              "      (W_V): Linear(in_features=512, out_features=512, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (3-4): 2 x LayerNorm((2000,), eps=1e-05, elementwise_affine=True)\n",
              "  (5): Linear(in_features=6144, out_features=2000, bias=True)\n",
              "  (6): Linear(in_features=2000, out_features=2000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log.info(\"Training model...\")\n",
        "\n",
        "model.train()\n",
        "batch_size = 80 # works on a T4 GPU free tier about 12 GB GPU RAM\n",
        "batch_size = 64 #A100 with num_head =12 == 34 GB\n",
        "N, seq_length = input_ids.shape\n",
        "log.info(f\"N= {N} seq_length= {seq_length}\")\n",
        "num_batches = N // batch_size\n",
        "\n",
        "for epoch in range(10):\n",
        "    for start_idx in range(0, N, batch_size):\n",
        "        end_idx = start_idx + batch_size\n",
        "        if end_idx > N:\n",
        "            break  # in case N not multiple of batch_size\n",
        "\n",
        "        # Slice out a batch\n",
        "        batch_input = input_ids[start_idx:end_idx, :]   # (B, seq_length)\n",
        "        batch_labels = labels[start_idx:end_idx, :]     # (B, seq_length)\n",
        "        if epoch == 0 and start_idx == 0:\n",
        "            log.info(f\"batch_input.shape={batch_input.shape}\")\n",
        "            log.info(f\"batch_labels.shape={batch_labels.shape}\")\n",
        "\n",
        "        # Move to GPU\n",
        "        batch_input = batch_input.to('cuda')\n",
        "        batch_labels = batch_labels.to('cuda')\n",
        "\n",
        "        # 1) Shift input & labels so model predicts next token\n",
        "        #    shape -> (B, seq_length-1)\n",
        "        trimmed_input = batch_input[:, :-1]\n",
        "        target_labels = batch_labels[:, 1:]\n",
        "        if epoch == 0 and start_idx == 0:\n",
        "            # take 10 tokens\n",
        "            log.info(\"Example input: %s\", sp.decode(trimmed_input[0].tolist()[:10]))\n",
        "            log.info(\"Example labels: %s\",sp.decode(target_labels[0].tolist()[:10]))\n",
        "\n",
        "        embedded_tokens = token_embedding(trimmed_input)\n",
        "        # shape remains (batch_size, seq_len, d_model)\n",
        "        pos_embedded_tokens = pos_encoding(embedded_tokens)\n",
        "        # Initialise the scores\n",
        "        # Initialize an empty list to store scores\n",
        "        head_outputs = []\n",
        "        # get attention and score from multihead attention- we are doing a very simple way\n",
        "        # in reality this could be parallelised by adding an extra dim to a matrxi and in one shot\n",
        "        for attention_mod in multihead_attention:\n",
        "            score,_ = attention_mod(pos_embedded_tokens)\n",
        "            head_outputs.append(score)\n",
        "        #Convert list of scores into a single tensor (concatenation or summation)\n",
        "        score = torch.cat(head_outputs, dim=-1)  # Concatenate along the last dimension\n",
        "        # todo - change this to score = torch.mean(torch.stack(head_outputs, dim=0), dim=0)  # Average over heads\n",
        "        #print(score.shape) # torch.Size([50, 999, 1024]) #  #last dim is dmodel*2 (num_heads)\n",
        "        # Predict the next word\n",
        "        hidden1 = prediction_layer1(score)  # Project to vocabulary size\n",
        "        hidden1 = layer_norm1(hidden1)         # add layer norm\n",
        "        logits = prediction_layer2(hidden1)  # through few linear layers\n",
        "        logits = layer_norm2(logits)      # add layer norm\n",
        "        # the last dimension of the output tensor represents the vocabulary size or the number of classes.\n",
        "        # Therefore, applying softmax along the last dimension (dim=-1)\n",
        "        predicted_probs = torch.softmax(logits, dim=-1)  # Get probabilities\n",
        "        # Get the predicted word (token ID)\n",
        "        predicted_token_id = torch.argmax(predicted_probs, dim=-1)\n",
        "        # Calculate the loss # crossentropy already does softmax inside\n",
        "        # If your input has 49 tokens, you predict 49 next tokens.\n",
        "        loss = loss_function(\n",
        "            logits.reshape(-1, vocab_size),\n",
        "            target_labels.reshape(-1)\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # We are not discarding the loss or ignoring it; rather, we’re enforcing a limit on the size of the update to avoid erratic jumps.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "        # print training progress occasionally\n",
        "        if (start_idx // batch_size) % 50 == 0:\n",
        "            log.info(\"[Epoch=%d | Batch=%d] loss=%.4f\", epoch+1, start_idx//batch_size, loss.item())\n",
        "        if loss.item() < 0.5:\n",
        "            break\n",
        "        # free gpu memory\n",
        "        del batch_input, batch_labels, trimmed_input, target_labels, logits\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    log.info(f\"---------Epoch {epoch+1:02d} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "\"\"\"# Use the trained model to predict\"\"\"\n",
        "\n",
        "# save the model weights\n",
        "torch.save(model.state_dict(), \"model_weights.pth\")\n",
        "log.info(\"Model weights saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymAQw3Rjx8z9",
        "outputId": "adc1e9a1-202b-4f95-9cdb-f1a0288b4fb5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "07-Feb-25 10:57:09 - Training model...\n",
            "07-Feb-25 10:57:09 - N= 24471 seq_length= 1000\n",
            "07-Feb-25 10:57:09 - batch_input.shape=torch.Size([64, 1000])\n",
            "07-Feb-25 10:57:09 - batch_labels.shape=torch.Size([64, 1000])\n",
            "07-Feb-25 10:57:09 - Example input: One day, a little girl named Lily found a \n",
            "07-Feb-25 10:57:09 - Example labels: day, a little girl named Lily found a need\n",
            "07-Feb-25 10:57:10 - [Epoch=1 | Batch=0] loss=8.0797\n",
            "07-Feb-25 10:58:08 - [Epoch=1 | Batch=50] loss=4.3915\n",
            "07-Feb-25 10:59:06 - [Epoch=1 | Batch=100] loss=3.9941\n",
            "07-Feb-25 11:00:05 - [Epoch=1 | Batch=150] loss=4.0190\n",
            "07-Feb-25 11:01:03 - [Epoch=1 | Batch=200] loss=3.6729\n",
            "07-Feb-25 11:02:02 - [Epoch=1 | Batch=250] loss=3.7241\n",
            "07-Feb-25 11:03:00 - [Epoch=1 | Batch=300] loss=3.7997\n",
            "07-Feb-25 11:03:58 - [Epoch=1 | Batch=350] loss=3.6950\n",
            "07-Feb-25 11:04:35 - ---------Epoch 01 | Loss: 3.6159\n",
            "07-Feb-25 11:04:36 - [Epoch=2 | Batch=0] loss=3.7283\n",
            "07-Feb-25 11:05:34 - [Epoch=2 | Batch=50] loss=3.3715\n",
            "07-Feb-25 11:06:32 - [Epoch=2 | Batch=100] loss=3.4167\n",
            "07-Feb-25 11:07:31 - [Epoch=2 | Batch=150] loss=3.6453\n",
            "07-Feb-25 11:08:29 - [Epoch=2 | Batch=200] loss=3.2991\n",
            "07-Feb-25 11:09:27 - [Epoch=2 | Batch=250] loss=3.4218\n",
            "07-Feb-25 11:10:26 - [Epoch=2 | Batch=300] loss=3.5630\n",
            "07-Feb-25 11:11:24 - [Epoch=2 | Batch=350] loss=3.4704\n",
            "07-Feb-25 11:12:00 - ---------Epoch 02 | Loss: 3.3899\n",
            "07-Feb-25 11:12:01 - [Epoch=3 | Batch=0] loss=3.5060\n",
            "07-Feb-25 11:13:00 - [Epoch=3 | Batch=50] loss=3.1414\n",
            "07-Feb-25 11:13:58 - [Epoch=3 | Batch=100] loss=3.2054\n",
            "07-Feb-25 11:14:56 - [Epoch=3 | Batch=150] loss=3.4613\n",
            "07-Feb-25 11:15:55 - [Epoch=3 | Batch=200] loss=3.0829\n",
            "07-Feb-25 11:16:53 - [Epoch=3 | Batch=250] loss=3.2272\n",
            "07-Feb-25 11:17:51 - [Epoch=3 | Batch=300] loss=3.3846\n",
            "07-Feb-25 11:18:50 - [Epoch=3 | Batch=350] loss=3.2959\n",
            "07-Feb-25 11:19:26 - ---------Epoch 03 | Loss: 3.2148\n",
            "07-Feb-25 11:19:27 - [Epoch=4 | Batch=0] loss=3.3246\n",
            "07-Feb-25 11:20:25 - [Epoch=4 | Batch=50] loss=2.9620\n",
            "07-Feb-25 11:21:23 - [Epoch=4 | Batch=100] loss=3.0279\n",
            "07-Feb-25 11:22:22 - [Epoch=4 | Batch=150] loss=3.2959\n",
            "07-Feb-25 11:23:20 - [Epoch=4 | Batch=200] loss=2.9109\n",
            "07-Feb-25 11:24:18 - [Epoch=4 | Batch=250] loss=3.0636\n",
            "07-Feb-25 11:25:16 - [Epoch=4 | Batch=300] loss=3.2255\n",
            "07-Feb-25 11:26:15 - [Epoch=4 | Batch=350] loss=3.1430\n",
            "07-Feb-25 11:26:51 - ---------Epoch 04 | Loss: 3.0644\n",
            "07-Feb-25 11:26:52 - [Epoch=5 | Batch=0] loss=3.1670\n",
            "07-Feb-25 11:27:50 - [Epoch=5 | Batch=50] loss=2.8173\n",
            "07-Feb-25 11:28:49 - [Epoch=5 | Batch=100] loss=2.8812\n",
            "07-Feb-25 11:29:47 - [Epoch=5 | Batch=150] loss=3.1566\n",
            "07-Feb-25 11:30:45 - [Epoch=5 | Batch=200] loss=2.7662\n",
            "07-Feb-25 11:31:43 - [Epoch=5 | Batch=250] loss=2.9304\n",
            "07-Feb-25 11:32:42 - [Epoch=5 | Batch=300] loss=3.0945\n",
            "07-Feb-25 11:33:40 - [Epoch=5 | Batch=350] loss=3.0168\n",
            "07-Feb-25 11:34:17 - ---------Epoch 05 | Loss: 2.9424\n",
            "07-Feb-25 11:34:18 - [Epoch=6 | Batch=0] loss=3.0371\n",
            "07-Feb-25 11:35:16 - [Epoch=6 | Batch=50] loss=2.6988\n",
            "07-Feb-25 11:36:15 - [Epoch=6 | Batch=100] loss=2.7669\n",
            "07-Feb-25 11:37:13 - [Epoch=6 | Batch=150] loss=3.0449\n",
            "07-Feb-25 11:38:12 - [Epoch=6 | Batch=200] loss=2.6521\n",
            "07-Feb-25 11:39:10 - [Epoch=6 | Batch=250] loss=2.8204\n",
            "07-Feb-25 11:40:09 - [Epoch=6 | Batch=300] loss=2.9902\n",
            "07-Feb-25 11:41:07 - [Epoch=6 | Batch=350] loss=2.9139\n",
            "07-Feb-25 11:41:43 - ---------Epoch 06 | Loss: 2.8424\n",
            "07-Feb-25 11:41:44 - [Epoch=7 | Batch=0] loss=2.9322\n",
            "07-Feb-25 11:42:43 - [Epoch=7 | Batch=50] loss=2.6020\n",
            "07-Feb-25 11:43:41 - [Epoch=7 | Batch=100] loss=2.6741\n",
            "07-Feb-25 11:44:39 - [Epoch=7 | Batch=150] loss=2.9530\n",
            "07-Feb-25 11:45:37 - [Epoch=7 | Batch=200] loss=2.5638\n",
            "07-Feb-25 11:46:36 - [Epoch=7 | Batch=250] loss=2.7329\n",
            "07-Feb-25 11:47:34 - [Epoch=7 | Batch=300] loss=2.9081\n",
            "07-Feb-25 11:48:33 - [Epoch=7 | Batch=350] loss=2.8309\n",
            "07-Feb-25 11:49:09 - ---------Epoch 07 | Loss: 2.7626\n",
            "07-Feb-25 11:49:10 - [Epoch=8 | Batch=0] loss=2.8502\n",
            "07-Feb-25 11:50:08 - [Epoch=8 | Batch=50] loss=2.5260\n",
            "07-Feb-25 11:51:07 - [Epoch=8 | Batch=100] loss=2.6001\n",
            "07-Feb-25 11:52:05 - [Epoch=8 | Batch=150] loss=2.8817\n",
            "07-Feb-25 11:53:03 - [Epoch=8 | Batch=200] loss=2.4927\n",
            "07-Feb-25 11:54:02 - [Epoch=8 | Batch=250] loss=2.6646\n",
            "07-Feb-25 11:55:00 - [Epoch=8 | Batch=300] loss=2.8406\n",
            "07-Feb-25 11:55:59 - [Epoch=8 | Batch=350] loss=2.7621\n",
            "07-Feb-25 11:56:35 - ---------Epoch 08 | Loss: 2.6986\n",
            "07-Feb-25 11:56:36 - [Epoch=9 | Batch=0] loss=2.7846\n",
            "07-Feb-25 11:57:34 - [Epoch=9 | Batch=50] loss=2.4635\n",
            "07-Feb-25 11:58:33 - [Epoch=9 | Batch=100] loss=2.5402\n",
            "07-Feb-25 11:59:31 - [Epoch=9 | Batch=150] loss=2.8243\n",
            "07-Feb-25 12:00:30 - [Epoch=9 | Batch=200] loss=2.4344\n",
            "07-Feb-25 12:01:28 - [Epoch=9 | Batch=250] loss=2.6076\n",
            "07-Feb-25 12:02:26 - [Epoch=9 | Batch=300] loss=2.7853\n",
            "07-Feb-25 12:03:25 - [Epoch=9 | Batch=350] loss=2.7054\n",
            "07-Feb-25 12:04:01 - ---------Epoch 09 | Loss: 2.6463\n",
            "07-Feb-25 12:04:02 - [Epoch=10 | Batch=0] loss=2.7317\n",
            "07-Feb-25 12:05:01 - [Epoch=10 | Batch=50] loss=2.4107\n",
            "07-Feb-25 12:05:59 - [Epoch=10 | Batch=100] loss=2.4895\n",
            "07-Feb-25 12:06:57 - [Epoch=10 | Batch=150] loss=2.7761\n",
            "07-Feb-25 12:07:56 - [Epoch=10 | Batch=200] loss=2.3845\n",
            "07-Feb-25 12:08:54 - [Epoch=10 | Batch=250] loss=2.5592\n",
            "07-Feb-25 12:09:53 - [Epoch=10 | Batch=300] loss=2.7384\n",
            "07-Feb-25 12:10:51 - [Epoch=10 | Batch=350] loss=2.6572\n",
            "07-Feb-25 12:11:27 - ---------Epoch 10 | Loss: 2.6012\n",
            "07-Feb-25 12:11:27 - Model weights saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# load the model for evaluation\n",
        "model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "# Test the generation function\n",
        "prompt = \"Bloom lived in a big garden\"\n",
        "\n",
        "generated_tokens = sp.encode(prompt, out_type=int)  # Tokenize input text\n",
        "\n",
        "# Convert to tensor\n",
        "input_tensor = torch.tensor(\n",
        "    generated_tokens, dtype=torch.long).unsqueeze(0)  # (1, seq_length)\n",
        "max_length = 100\n",
        "for _ in range(max_length):\n",
        "    # Get embedding\n",
        "    embedded_tokens = token_embedding(input_tensor.to('cuda'))\n",
        "    # Get attention and score\n",
        "    # shape remains (batch_size, seq_len, d_model)\n",
        "    pos_embedded_tokens = pos_encoding(embedded_tokens)\n",
        "    head_outputs = []\n",
        "    # get attention and score from multihead attention\n",
        "    for attention_mod in multihead_attention:\n",
        "        score,_ = attention_mod(pos_embedded_tokens)\n",
        "        head_outputs.append(score)\n",
        "    #Convert list of scores into a single tensor (concatenation or summation)\n",
        "    score = torch.cat(head_outputs, dim=-1)  # Concatenate along the last dimension #better to do mean\n",
        "    #print(score.shape) # torch.Size([50, 999, 1024]) #  #last dim is dmodel*2 (num_heads)\n",
        "    # Predict the next word\n",
        "    hidden1 = prediction_layer1(score)  # Project to vocabulary size\n",
        "    hidden1 = layer_norm1(hidden1)         # add layer norm\n",
        "    logits = prediction_layer2(hidden1)  # through few linear layers\n",
        "    logits = layer_norm2(logits)      # add layer norm\n",
        "    predicted_probs = torch.softmax(logits, dim=-1)  # Get probabilities\n",
        "    # Get the last token's logits (for autoregressive prediction)\n",
        "    next_token_logits = predicted_probs[:, -1, :]  # Shape: (1, vocab_size)\n",
        "    # Convert logits to token probabilities\n",
        "    next_token_id = torch.argmax(next_token_logits, dim=-1)  # (1,)\n",
        "    # Append new token\n",
        "    generated_tokens.append(next_token_id.item())\n",
        "    # Stop if we generate an EOS token (optional)\n",
        "    if next_token_id.item() == sp.eos_id():  # Ensure your tokenizer has an EOS token\n",
        "        break\n",
        "\n",
        "    # Update input tensor with new token for next iteration\n",
        "    input_tensor = torch.tensor(\n",
        "        generated_tokens, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "# Decode generated token IDs back to text\n",
        "generated_text = sp.decode(generated_tokens)\n",
        "log.info(f\"Generated Text={generated_text}\")"
      ],
      "metadata": {
        "id": "84X0ig2Xxr9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "719db5af-a19a-4286-8421-9d0b9b249dea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-bdde7f35a530>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
            "07-Feb-25 12:23:05 - Generated Text=Bloom lived in a big garden with a big smile on her face. She was so happy and thanked her mom for helping her. She was so happy and hugged her mom and said, \"Thank you, mom. You are very kind.\" But, Brownie and Brownie careful with the big smile on her face. She was so happy and thanked her mom for helping her mom and dad. She hugged her mom and said, \"Thank you, Bye. You are very kind.\" Brow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: copy weitghts to drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xsJ-nQNFYESw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model weights to Google Drive\n",
        "!cp model_weights.pth /content/drive/My\\ Drive/Colab\\ Notebooks/\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-hgD6arBaRA",
        "outputId": "b207607b-8c1b-4879-c71e-14047cfc9a04"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp multihead_transformer.log /content/drive/My\\ Drive/Colab\\ Notebooks/"
      ],
      "metadata": {
        "id": "ahSu_HMVYGBY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt):\n",
        "  generated_tokens = sp.encode(prompt, out_type=int)  # Tokenize input text\n",
        "  # Convert to tensor\n",
        "  input_tensor = torch.tensor(\n",
        "      generated_tokens, dtype=torch.long).unsqueeze(0)  # (1, seq_length)\n",
        "  max_length = 100\n",
        "  for _ in range(max_length):\n",
        "      # Get embedding\n",
        "      embedded_tokens = token_embedding(input_tensor.to('cuda'))\n",
        "      # Get attention and score\n",
        "      # shape remains (batch_size, seq_len, d_model)\n",
        "      pos_embedded_tokens = pos_encoding(embedded_tokens)\n",
        "      head_outputs = []\n",
        "      # get attention and score from multihead attention\n",
        "      for attention_mod in multihead_attention:\n",
        "          score,_ = attention_mod(pos_embedded_tokens)\n",
        "          head_outputs.append(score)\n",
        "      #Convert list of scores into a single tensor (concatenation or summation)\n",
        "      score = torch.cat(head_outputs, dim=-1)  # Concatenate along the last dimension #better to do mean\n",
        "      #print(score.shape) # torch.Size([50, 999, 1024]) #  #last dim is dmodel*2 (num_heads)\n",
        "      # Predict the next word\n",
        "      hidden1 = prediction_layer1(score)  # Project to vocabulary size\n",
        "      hidden1 = layer_norm1(hidden1)         # add layer norm\n",
        "      logits = prediction_layer2(hidden1)  # through few linear layers\n",
        "      logits = layer_norm2(logits)      # add layer norm\n",
        "      predicted_probs = torch.softmax(logits, dim=-1)  # Get probabilities\n",
        "      # Get the last token's logits (for autoregressive prediction)\n",
        "      next_token_logits = predicted_probs[:, -1, :]  # Shape: (1, vocab_size)\n",
        "      # Convert logits to token probabilities\n",
        "      next_token_id = torch.argmax(next_token_logits, dim=-1)  # (1,)\n",
        "      # Append new token\n",
        "      generated_tokens.append(next_token_id.item())\n",
        "      # Stop if we generate an EOS token (optional)\n",
        "      if next_token_id.item() == sp.eos_id():  # Ensure your tokenizer has an EOS token\n",
        "          break\n",
        "\n",
        "      # Update input tensor with new token for next iteration\n",
        "      input_tensor = torch.tensor(\n",
        "          generated_tokens, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "  # Decode generated token IDs back to text\n",
        "  generated_text = sp.decode(generated_tokens)\n",
        "  return generated_text\n"
      ],
      "metadata": {
        "id": "AHP5H5LKYayb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the generation function\n",
        "prompt = \"The Cat sat on the\"\n",
        "generated_text = generate(prompt)\n",
        "log.info(f\"Generated Text={generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_8bweqsYznM",
        "outputId": "967207de-30b9-489d-8ec2-8b87523645a2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "07-Feb-25 12:30:04 - Generated Text=The Cat sat on the table and the chair. The chased the chair. The chased the chair and the chair was so happy. The chair was so happy and thanked the chair for the chair and the chair and the chair was so happy to have the chair and the chair was so glad to have such a wonderful time. Once upon a time there was a little girl named Amy. She was three years old and loved to play in the chair and it \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding temperature based sampling as per chatgpt\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(prompt, max_length=100, temperature=1.0):\n",
        "    generated_tokens = sp.encode(prompt, out_type=int)  # Tokenize input text\n",
        "    input_tensor = torch.tensor(generated_tokens, dtype=torch.long).unsqueeze(0).to('cuda')  # (1, seq_length)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Get embedding\n",
        "        embedded_tokens = token_embedding(input_tensor)\n",
        "        pos_embedded_tokens = pos_encoding(embedded_tokens)\n",
        "\n",
        "        # Multi-head attention\n",
        "        head_outputs = [attention_mod(pos_embedded_tokens)[0] for attention_mod in multihead_attention]\n",
        "        score = torch.cat(head_outputs, dim=-1)  # Use mean instead of cat if needed\n",
        "\n",
        "        # Predict next token\n",
        "        hidden1 = layer_norm1(prediction_layer1(score))\n",
        "        logits = layer_norm2(prediction_layer2(hidden1))\n",
        "\n",
        "        # Get last token's logits (for autoregressive prediction)\n",
        "        next_token_logits = logits[:, -1, :]  # Shape: (1, vocab_size)\n",
        "\n",
        "        # Apply temperature-based scaling before sampling\n",
        "        scaled_logits = next_token_logits / temperature  #  Adjust randomness\n",
        "        probabilities = F.softmax(scaled_logits, dim=-1)  # Convert to probabilities\n",
        "\n",
        "        # Sample from the probability distribution\n",
        "        next_token_id = torch.multinomial(probabilities, num_samples=1).item()  #  Random sampling\n",
        "\n",
        "        # Append new token\n",
        "        generated_tokens.append(next_token_id)\n",
        "\n",
        "        # Stop if we generate an EOS token (optional)\n",
        "        if next_token_id == sp.eos_id():  # Ensure your tokenizer has an EOS token\n",
        "            break\n",
        "\n",
        "        # Update input tensor with new token for next iteration\n",
        "        input_tensor = torch.tensor(generated_tokens, dtype=torch.long).unsqueeze(0).to('cuda')\n",
        "\n",
        "    # Decode generated token IDs back to text\n",
        "    return sp.decode(generated_tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "Z6LUITI_Zy6-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Controlling Temperature**\n",
        "\n",
        "| **Temperature (T)** | **Effect** |\n",
        "|--------------------|------------------------------|\n",
        "| **T = 0.1**  | Almost deterministic, like `argmax` |\n",
        "| **T = 0.7**  | Balanced creativity vs. coherence |\n",
        "| **T = 1.0**  | Standard sampling (default) |\n",
        "| **T = 1.5+**  | Very creative but may generate gibberish |\n"
      ],
      "metadata": {
        "id": "ps7UEAjPaPaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the generation function\n",
        "prompt = \"The Cat sat on the\"\n",
        "generated_text = generate(prompt,temperature=0.4)\n",
        "log.info(f\"Generated Text={generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5frCgCE3Z8y2",
        "outputId": "e51f8364-fdd9-4870-932e-078db86c9625"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "07-Feb-25 12:39:47 - Generated Text=The Cat sat on the table. The little girl was so happy and she had a great time playing with her new toy. She was so happy and she had a new friend. She was so happy to have her new friend and she was proud of her new friend. Once upon a time there was a little girl who loved to play in the park. One day she was walking in the park when she saw a big tree in her garden. She was so excited she wanted to show her friends her friends to play with. \n"
          ]
        }
      ]
    }
  ]
}